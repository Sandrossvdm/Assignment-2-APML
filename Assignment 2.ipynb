{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indian-mauritius",
   "metadata": {},
   "source": [
    "# Assignment 2 - Predictive Process Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-oracle",
   "metadata": {},
   "source": [
    "**Due: Wednesday, 11 December, 2024 at 17:00 CET**\n",
    "\n",
    "In this assignment, you will learn to train a regression model to predict the remaining time of a process. Additionally, you will demonstrate your ability to evaluate the model's performance and discuss the results in a report. The learning objectives of this assignment are to:\n",
    "\n",
    "- Apply data cleaning, data transformation, and feature encoding techniques to preprocess event data.\n",
    "- Use regression models to predict the remaining time of ongoing cases.\n",
    "- Calculate model performance metrics (e.g., MAE, MSE, RMSE, \\(R^2\\), etc.).\n",
    "- Refine the experimental design to compare the performance of different preprocessing and encoding methods.\n",
    "- Reflect on the differences between various methods and their effect on the model performance.\n",
    "\n",
    "\n",
    "## Tasks Overview\n",
    "\n",
    "This assignment includes six tasks:\n",
    "\n",
    "1. **Data Exploration:** Perform data exploration to understand the dataset.\n",
    "2. **Data Preprocessing and Trace Encoding:** Apply data preprocessing and trace encoding (covered during Lectures 5 and 7).\n",
    "3. **Regression Model Training:** Select a regression algorithm of interest and train a regression model (a regressor) to forecast the remaining time of each case after each event (see Lecture 6)\n",
    "4. **Prefix-Length Buckets:** Create buckets of different prefix lengths and train a separate regressor for each bucket (covered during Lectures 5 and 7).\n",
    "5. **Alternative Methods:** Revisit your design decisions and investigate two additional methods that may improve model performance.\n",
    "6. **Evaluation:** Evaluate the results.\n",
    "\n",
    "### Notes\n",
    "\n",
    "- You may reuse your code from Tasks 2 and 3 in Tasks 4 and 5.\n",
    "- For Task 6 and your report, ensure that you save all the calculated metrics (MAE, MSE, RMSE, and \\(R^2\\)) in previous tasks. Save these metrics in a list or dictionary to facilitate easy evaluation and comparison of results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-pixel",
   "metadata": {},
   "source": [
    "## Task 1: Exploring the data set\n",
    "\n",
    "\n",
    "\n",
    "### Data set: Sepsis\n",
    "\n",
    "Import the file *Complete Sepsis.csv* to load the Sepsis data set. This real-life event log contains events of sepsis cases from a hospital. Sepsis is a life threatening condition typically caused by an infection. One case represents a patient's pathway through the treatment process. The events were recorded by the ERP (Enterprise Resource Planning) system of the hospital. The original data set contains about 1000 cases with in total 15,000 events that were recorded for 16 different activities. Moreover, 39 data attributes are recorded, e.g., the group responsible for the activity, the results of tests and information from checklists. \n",
    "\n",
    "Additional information about the data can be found :\n",
    "- https://data.4tu.nl/articles/dataset/Sepsis_Cases_-_Event_Log/12707639\n",
    "- http://ceur-ws.org/Vol-1859/bpmds-08-paper.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "appropriate-michigan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Variant index         Age   Leucocytes          CRP   LacticAcid\n",
      "count   13333.000000  777.000000  3075.000000  2884.000000  1281.000000\n",
      "mean      360.079127   71.833977    12.985463   114.368585     1.902420\n",
      "std       214.255001   15.573718    15.512797    86.366989     1.442281\n",
      "min         1.000000   20.000000     0.200000     5.000000     0.200000\n",
      "25%       176.000000   60.000000     7.700000    44.000000     1.100000\n",
      "50%       360.000000   75.000000    10.900000    96.000000     1.500000\n",
      "75%       547.000000   85.000000    15.000000   163.000000     2.200000\n",
      "max       728.000000   90.000000   381.300000   573.000000    14.900000\n",
      "\n",
      " Dataset columns \n",
      " Index(['Case ID', 'Activity', 'Complete Timestamp', 'Variant', 'Variant index',\n",
      "       'lifecycle:transition', 'org:group', 'InfectionSuspected',\n",
      "       'DiagnosticBlood', 'DisfuncOrg', 'SIRSCritTachypnea', 'Hypotensie',\n",
      "       'SIRSCritHeartRate', 'Infusion', 'DiagnosticArtAstrup', 'Age',\n",
      "       'DiagnosticIC', 'DiagnosticSputum', 'DiagnosticLiquor',\n",
      "       'DiagnosticOther', 'SIRSCriteria2OrMore', 'DiagnosticXthorax',\n",
      "       'SIRSCritTemperature', 'DiagnosticUrinaryCulture', 'SIRSCritLeucos',\n",
      "       'Oligurie', 'DiagnosticLacticAcid', 'Diagnose', 'Hypoxie',\n",
      "       'DiagnosticUrinarySediment', 'DiagnosticECG', 'Leucocytes', 'CRP',\n",
      "       'LacticAcid'],\n",
      "      dtype='object')\n",
      "\n",
      " Unique activities \n",
      " ['ER Registration' 'Leucocytes' 'CRP' 'LacticAcid' 'ER Triage'\n",
      " 'ER Sepsis Triage' 'IV Liquid' 'IV Antibiotics' 'Admission NC'\n",
      " 'Release A' 'Return ER' 'Admission IC' 'Release B' 'Release C'\n",
      " 'Release D' 'Release E']\n",
      "\n",
      " The data shape is (13333, 34)\n",
      "unique Case-ID's ['A' 'B' 'C' 'D' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'N' 'O' 'S' 'T' 'U' 'V' 'W'\n",
      " 'X' 'Z' 'BA' 'DA' 'EA' 'GA' 'HA' 'JA' 'KA' 'MA' nan 'QA' 'SA' 'TA' 'UA'\n",
      " 'VA' 'WA' 'CB' 'DB' 'GB' 'HB' 'IB' 'KB' 'LB' 'NB' 'OB' 'PB' 'QB' 'RB'\n",
      " 'SB' 'TB' 'VB' 'XB' 'YB' 'ZB' 'AC' 'CC' 'DC' 'EC' 'FC' 'GC' 'IC' 'JC'\n",
      " 'KC' 'LC' 'MC' 'OC' 'QC' 'RC' 'TC' 'UC' 'VC' 'WC' 'AD' 'BD' 'DD' 'ED'\n",
      " 'GD' 'HD' 'JD' 'KD' 'LD' 'MD' 'ND' 'OD' 'QD' 'SD' 'TD' 'UD' 'VD' 'XD'\n",
      " 'ZD' 'AE' 'BE' 'CE' 'EE' 'FE' 'GE' 'HE' 'IE' 'JE' 'KE' 'LE' 'ME' 'QE'\n",
      " 'SE' 'UE' 'VE' 'WE' 'YE' 'ZE' 'AF' 'BF' 'CF' 'EF' 'GF' 'HF' 'IF' 'KF'\n",
      " 'NF' 'PF' 'QF' 'SF' 'UF' 'VF' 'WF' 'XF' 'YF' 'ZF' 'BG' 'CG' 'DG' 'FG'\n",
      " 'GG' 'HG' 'IG' 'JG' 'KG' 'LG' 'OG' 'PG' 'QG' 'RG' 'TG' 'VG' 'WG' 'YG'\n",
      " 'ZG' 'AH' 'BH' 'CH' 'DH' 'EH' 'FH' 'HH' 'IH' 'JH' 'KH' 'LH' 'NH' 'OH'\n",
      " 'QH' 'SH' 'TH' 'XH' 'YH' 'ZH' 'AI' 'BI' 'CI' 'DI' 'EI' 'GI' 'HI' 'II'\n",
      " 'KI' 'LI' 'NI' 'PI' 'QI' 'RI' 'SI' 'TI' 'UI' 'VI' 'WI' 'XI' 'YI' 'ZI'\n",
      " 'AJ' 'BJ' 'CJ' 'DJ' 'EJ' 'FJ' 'GJ' 'HJ' 'IJ' 'JJ' 'KJ' 'LJ' 'MJ' 'NJ'\n",
      " 'OJ' 'PJ' 'QJ' 'SJ' 'TJ' 'UJ' 'VJ' 'XJ' 'YJ' 'ZJ' 'BK' 'DK' 'EK' 'FK'\n",
      " 'GK' 'HK' 'JK' 'KK' 'LK' 'MK' 'NK' 'OK' 'PK' 'QK' 'RK' 'SK' 'UK' 'VK'\n",
      " 'WK' 'YK' 'ZK' 'AL' 'BL' 'DL' 'EL' 'GL' 'HL' 'IL' 'JL' 'LL' 'ML' 'OL'\n",
      " 'RL' 'SL' 'TL' 'VL' 'WL' 'XL' 'YL' 'ZL' 'CM' 'EM' 'FM' 'GM' 'HM' 'IM'\n",
      " 'JM' 'KM' 'LM' 'MM' 'NM' 'OM' 'QM' 'RM' 'SM' 'VM' 'WM' 'XM' 'YM' 'ZM'\n",
      " 'BN' 'CN' 'FN' 'GN' 'HN' 'JN' 'KN' 'LN' 'MN' 'NN' 'ON' 'QN' 'RN' 'TN'\n",
      " 'VN' 'WN' 'XN' 'AO' 'CO' 'DO' 'EO' 'FO' 'GO' 'HO' 'IO' 'JO' 'KO' 'MO'\n",
      " 'NO' 'OO' 'PO' 'QO' 'SO' 'TO' 'VO' 'WO' 'XO' 'YO' 'ZO' 'BP' 'CP' 'DP'\n",
      " 'GP' 'HP' 'IP' 'KP' 'LP' 'MP' 'NP' 'QP' 'RP' 'TP' 'UP' 'VP' 'WP' 'XP'\n",
      " 'YP' 'ZP' 'AQ' 'BQ' 'CQ' 'EQ' 'FQ' 'GQ' 'HQ' 'IQ' 'JQ' 'KQ' 'LQ' 'MQ'\n",
      " 'OQ' 'PQ' 'QQ' 'SQ' 'TQ' 'UQ' 'VQ' 'WQ' 'XQ' 'YQ' 'AR' 'BR' 'CR' 'DR'\n",
      " 'HR' 'JR' 'LR' 'MR' 'NR' 'OR' 'PR' 'RR' 'SR' 'TR' 'UR' 'VR' 'WR' 'YR'\n",
      " 'ZR' 'AS' 'BS' 'DS' 'ES' 'GS' 'HS' 'IS' 'JS' 'KS' 'LS' 'MS' 'OS' 'PS'\n",
      " 'QS' 'RS' 'SS' 'TS' 'US' 'VS' 'XS' 'YS' 'ZS' 'BT' 'CT' 'DT' 'ET' 'FT'\n",
      " 'GT' 'HT' 'IT' 'JT' 'KT' 'LT' 'MT' 'NT' 'OT' 'PT' 'RT' 'ST' 'UT' 'VT'\n",
      " 'WT' 'XT' 'YT' 'ZT' 'AU' 'BU' 'CU' 'EU' 'FU' 'GU' 'IU' 'JU' 'LU' 'MU'\n",
      " 'NU' 'OU' 'PU' 'QU' 'RU' 'TU' 'UU' 'VU' 'WU' 'ZU' 'AV' 'BV' 'CV' 'GV'\n",
      " 'HV' 'JV' 'KV' 'LV' 'MV' 'NV' 'PV' 'SV' 'TV' 'UV' 'WV' 'YV' 'ZV' 'BW'\n",
      " 'CW' 'DW' 'EW' 'FW' 'IW' 'JW' 'KW' 'LW' 'MW' 'NW' 'OW' 'PW' 'QW' 'RW'\n",
      " 'SW' 'TW' 'UW' 'VW' 'WW' 'XW' 'YW' 'ZW' 'BX' 'CX' 'DX' 'EX' 'FX' 'GX'\n",
      " 'HX' 'JX' 'KX' 'LX' 'MX' 'NX' 'OX' 'QX' 'SX' 'TX' 'XX' 'YX' 'ZX' 'AY'\n",
      " 'CY' 'DY' 'EY' 'FY' 'GY' 'HY' 'IY' 'JY' 'OY' 'QY' 'RY' 'SY' 'VY' 'XY'\n",
      " 'YY' 'AZ' 'BZ' 'CZ' 'DZ' 'FZ' 'HZ' 'IZ' 'KZ' 'LZ' 'MZ' 'NZ' 'PZ' 'QZ'\n",
      " 'RZ' 'UZ' 'WZ' 'XZ' 'YZ' 'ZZ' 'AAA' 'BAA' 'DAA' 'EAA' 'FAA' 'GAA' 'HAA'\n",
      " 'IAA' 'JAA' 'KAA' 'MAA' 'OAA' 'QAA' 'RAA' 'SAA' 'TAA' 'VAA' 'WAA' 'XAA'\n",
      " 'YAA' 'ZAA' 'ABA' 'BBA' 'CBA' 'DBA' 'EBA' 'FBA' 'GBA' 'HBA' 'IBA' 'KBA'\n",
      " 'LBA' 'MBA' 'OBA' 'PBA' 'QBA' 'RBA' 'SBA' 'TBA' 'UBA' 'WBA' 'XBA' 'YBA'\n",
      " 'ZBA' 'BCA' 'DCA' 'ECA' 'FCA' 'GCA' 'JCA' 'KCA' 'NCA' 'QCA' 'RCA' 'SCA'\n",
      " 'TCA' 'VCA' 'WCA' 'XCA' 'BDA' 'DDA' 'EDA' 'FDA' 'GDA' 'HDA' 'IDA' 'LDA'\n",
      " 'NDA' 'ODA' 'PDA' 'QDA' 'SDA' 'TDA' 'UDA' 'VDA' 'WDA' 'YDA' 'ZDA' 'CEA'\n",
      " 'DEA' 'EEA' 'FEA' 'GEA' 'IEA' 'LEA' 'MEA' 'NEA' 'OEA' 'PEA' 'SEA' 'TEA'\n",
      " 'XEA' 'YEA' 'ZEA' 'AFA' 'BFA' 'CFA' 'DFA' 'GFA' 'JFA' 'KFA' 'LFA' 'MFA'\n",
      " 'PFA' 'QFA' 'RFA' 'SFA' 'VFA' 'WFA' 'XFA' 'YFA' 'ZFA' 'AGA' 'BGA' 'CGA'\n",
      " 'DGA' 'EGA' 'HGA' 'IGA' 'JGA' 'MGA' 'NGA' 'OGA' 'PGA' 'RGA' 'SGA' 'TGA'\n",
      " 'UGA' 'WGA' 'XGA' 'YGA' 'ZGA' 'AHA' 'BHA' 'CHA' 'DHA' 'EHA' 'FHA' 'GHA'\n",
      " 'HHA' 'IHA' 'JHA' 'KHA' 'LHA' 'MHA' 'NHA' 'OHA' 'QHA' 'RHA' 'SHA' 'THA'\n",
      " 'WHA' 'YHA' 'ZHA' 'AIA' 'BIA' 'EIA' 'FIA' 'HIA' 'IIA' 'KIA' 'LIA' 'MIA'\n",
      " 'NIA' 'OIA' 'PIA' 'QIA' 'RIA' 'TIA' 'UIA' 'VIA' 'YIA' 'BJA' 'CJA' 'DJA'\n",
      " 'EJA' 'FJA' 'GJA' 'HJA' 'JJA' 'NJA' 'SJA' 'TJA' 'WJA' 'XJA' 'YJA' 'ZJA'\n",
      " 'BKA' 'CKA' 'DKA' 'EKA' 'FKA' 'HKA' 'JKA' 'KKA' 'LKA' 'MKA' 'NKA' 'OKA'\n",
      " 'PKA' 'QKA' 'RKA' 'TKA' 'UKA' 'VKA' 'WKA' 'XKA' 'YKA' 'ZKA' 'ALA' 'BLA'\n",
      " 'ELA' 'GLA' 'HLA' 'ILA' 'JLA' 'KLA' 'OLA' 'PLA' 'RLA' 'SLA' 'ULA' 'WLA'\n",
      " 'YLA' 'ZLA' 'AMA' 'BMA' 'EMA' 'FMA' 'GMA' 'JMA' 'KMA' 'LMA' 'NMA' 'OMA'\n",
      " 'PMA' 'QMA' 'SMA' 'TMA' 'UMA' 'YMA' 'ZMA' 'ANA' 'BNA' 'CNA' 'GNA' 'HNA'\n",
      " 'KNA']\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "\n",
    "# Import data\n",
    "# TODO: Change the file path to your file path\n",
    "data_Sepsis = pd.read_csv(\"./Complete Sepsis.csv\", sep=\",\")\n",
    "\n",
    "# create an overview of the data\n",
    "print(data_Sepsis.describe())\n",
    "print(f\"\\n Dataset columns \\n\",data_Sepsis.columns)\n",
    "# print(data_Sepsis.head())\n",
    "\n",
    "# Print the activities that occurred in the process\n",
    "print(f\"\\n Unique activities \\n\",data_Sepsis['Activity'].unique())\n",
    "print(f\"\\n The data shape is\",data_Sepsis.shape)\n",
    "# Print the cases (patients) that are following the process\n",
    "print(f\"unique Case-ID's\", data_Sepsis['Case ID'].unique())\n",
    "\n",
    "# for variable in data_Sepsis.columns:\n",
    "#     print(variable)\n",
    "#     print(data_Sepsis[variable].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ddf7d3-ab12-4bc1-b052-5fa539115f0a",
   "metadata": {},
   "source": [
    "### Create the Labels by Calculating the Remaining Time\n",
    "\n",
    "To forecast the remaining time for each patient in the hospital, we group the events by patient, use the completion time of each patient (i.e., the timestamp of the last event for each patient), and calculate the time difference between the current event and the completion time. This is done for each event. As a result, we now have our labels, which indicate how long a patient will remain in the treatment process for each event.\n",
    "\n",
    "To help you get started, we created the target variable *remaining_time(days)* for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "445dabbf-00b3-4ebb-9bd1-31b6e8484bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_Sepsis.copy()\n",
    "\n",
    "# Convert the timestamp column to datetime\n",
    "df['Complete Timestamp'] = pd.to_datetime(df['Complete Timestamp'])\n",
    "\n",
    "# Find the completion time for each case\n",
    "completion_times = df.groupby('Case ID')['Complete Timestamp'].max().rename('completion_time')\n",
    "\n",
    "# Merge completion time back into the original DataFrame\n",
    "df = df.merge(completion_times, on='Case ID')\n",
    "\n",
    "# Calculate the remaining time for each event\n",
    "df['remaining_time'] = df['completion_time'] - df['Complete Timestamp']\n",
    "\n",
    "\n",
    "# Calculate the remaining time for each event in days\n",
    "df['remaining_time(days)'] = df['remaining_time'].dt.total_seconds()/60/60/24\n",
    "\n",
    "# Retain rows where the remaining time is larger than 0 days. \n",
    "df = df[df['remaining_time(days)'] > 0]\n",
    "\n",
    "# Drop the completion_time column to avoid information leakage\n",
    "df = df.drop(columns=['completion_time'])\n",
    "\n",
    "# Drop the remaining_time column to avoid information leakage\n",
    "df = df.drop(columns=['remaining_time'])\n",
    "\n",
    "# Display the result\n",
    "label_column = 'remaining_time(days)'\n",
    "# print(df['remaining_time(days)'].describe())\n",
    "\n",
    "# Fill in the column names of case id, activity, and time stamps\n",
    "column_Sepsis_CaseID = 'Case ID' \n",
    "column_Sepsis_Activity = 'Activity'\n",
    "column_Sepsis_Timestamps = 'Complete Timestamp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-queen",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Exploratory data analysis\n",
    "\n",
    "For the data set, create 2-3 figures/tables that help you understand the data \n",
    "\n",
    "Note that some of these variables are categorical variables and some are numberical. Additionally, some of the variables have missing values. Think/discuss how would you preprocess these variables.\n",
    "\n",
    "\n",
    "Make sure to at least check each variable's data type and understand their distribution. \n",
    "\n",
    "*For creating data visualizations, you may consider using the matplot library and visit the [matplot gallery](https://matplotlib.org/stable/gallery/index.html) for inspiration (e.g., histograms for distribution, or heatmaps for feature correlation).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dangerous-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# TODO: plot figure(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-imperial",
   "metadata": {},
   "source": [
    "### (Optional) 1.2 Process Discovery and Visualization \n",
    "\n",
    "This is an optional task to show you how process discovery and visualizaion can be deployed using the pm4py library. \n",
    "\n",
    "(*The following code requires the graphviz library to be installed. If you have issues with installing the graphviz, you may try to follow the instructions on Install GraphViz on the [pm4py](https://pm4py.fit.fraunhofer.de/install-page) install page*)\n",
    "\n",
    "The following code:\n",
    "- fill in the columns for case id, activity, and timestamps\n",
    "- convert the data set into an event log\n",
    "- discover a Directly-follows graph (DFG) and a process model for each event log. \n",
    "- you may use the discovered process model in your report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "reflected-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have brew installed, install graphviz brew install graphviz\n",
    "# Then install pm4py library and graphviz library using pip install \n",
    "# !pip install -U pm4py\n",
    "# !conda install graphviz\n",
    "# !conda install python-graphviz\n",
    "# import pm4py\n",
    "\n",
    "\n",
    "# data_Sepsis[column_Sepsis_CaseID] = data_Sepsis[column_Sepsis_CaseID].astype(str)\n",
    "\n",
    "# data_Sepsis_copy = data_Sepsis.copy()\n",
    "# data_Sepsis_copy['Complete Timestamp'] = pd.to_datetime(data_Sepsis_copy['Complete Timestamp'])\n",
    "\n",
    "# # Convert the data to an event log\n",
    "# log_Sepsis = pm4py.format_dataframe(data_Sepsis_copy, case_id=column_Sepsis_CaseID, activity_key=column_Sepsis_Activity, timestamp_key=column_Sepsis_Timestamps)\n",
    "\n",
    "# # Set the log to be the one that you are interested\n",
    "# log = log_Sepsis\n",
    "\n",
    "# # Create a Directly-Follows Graph (DFG) and plot this graph\n",
    "# dfg, start_activities, end_activities = pm4py.discover_dfg(log)\n",
    "# pm4py.view_dfg(dfg, start_activities, end_activities)\n",
    "\n",
    "# # Discover a Process Model using Inductive Miner and plot this BPMN model\n",
    "# process_tree = pm4py.discover_process_tree_inductive(log)\n",
    "# bpmn_model = pm4py.convert_to_bpmn(process_tree)\n",
    "# pm4py.view_bpmn(bpmn_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-distinction",
   "metadata": {},
   "source": [
    "## Task 2: Preprocessing and Trace Encoding\n",
    "\n",
    "### 2.1 Data preprocessing\n",
    "\n",
    "In the previous data exploration task, you gathered some initial insights about the dataset. Based on your observations during data exploration, decide which preprocessing steps are necessary (e.g., handling missing values, encoding categorical variables, scaling numerical features, etc.) and implement them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e1725-dd60-4a79-aefa-5d83e7af69cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "Case ID                          0\n",
      "Activity                         0\n",
      "Complete Timestamp               0\n",
      "Variant                          0\n",
      "Variant index                    0\n",
      "lifecycle:transition             0\n",
      "org:group                        0\n",
      "InfectionSuspected           11757\n",
      "DiagnosticBlood              11757\n",
      "DisfuncOrg                   11757\n",
      "SIRSCritTachypnea            11757\n",
      "Hypotensie                   11757\n",
      "SIRSCritHeartRate            11757\n",
      "Infusion                     11757\n",
      "DiagnosticArtAstrup          11757\n",
      "Age                          11757\n",
      "DiagnosticIC                 11757\n",
      "DiagnosticSputum             11757\n",
      "DiagnosticLiquor             11757\n",
      "DiagnosticOther              11757\n",
      "SIRSCriteria2OrMore          11757\n",
      "DiagnosticXthorax            11757\n",
      "SIRSCritTemperature          11757\n",
      "DiagnosticUrinaryCulture     11757\n",
      "SIRSCritLeucos               11757\n",
      "Oligurie                     11757\n",
      "DiagnosticLacticAcid         11757\n",
      "Diagnose                     11762\n",
      "Hypoxie                      11757\n",
      "DiagnosticUrinarySediment    11757\n",
      "DiagnosticECG                11757\n",
      "Leucocytes                    9467\n",
      "CRP                           9654\n",
      "LacticAcid                   11254\n",
      "remaining_time(days)             0\n",
      "dtype: int64\n",
      "\n",
      "Percentage of missing values per column:\n",
      "Case ID                       0.000000\n",
      "Activity                      0.000000\n",
      "Complete Timestamp            0.000000\n",
      "Variant                       0.000000\n",
      "Variant index                 0.000000\n",
      "lifecycle:transition          0.000000\n",
      "org:group                     0.000000\n",
      "InfectionSuspected           93.808346\n",
      "DiagnosticBlood              93.808346\n",
      "DisfuncOrg                   93.808346\n",
      "SIRSCritTachypnea            93.808346\n",
      "Hypotensie                   93.808346\n",
      "SIRSCritHeartRate            93.808346\n",
      "Infusion                     93.808346\n",
      "DiagnosticArtAstrup          93.808346\n",
      "Age                          93.808346\n",
      "DiagnosticIC                 93.808346\n",
      "DiagnosticSputum             93.808346\n",
      "DiagnosticLiquor             93.808346\n",
      "DiagnosticOther              93.808346\n",
      "SIRSCriteria2OrMore          93.808346\n",
      "DiagnosticXthorax            93.808346\n",
      "SIRSCritTemperature          93.808346\n",
      "DiagnosticUrinaryCulture     93.808346\n",
      "SIRSCritLeucos               93.808346\n",
      "Oligurie                     93.808346\n",
      "DiagnosticLacticAcid         93.808346\n",
      "Diagnose                     93.848241\n",
      "Hypoxie                      93.808346\n",
      "DiagnosticUrinarySediment    93.808346\n",
      "DiagnosticECG                93.808346\n",
      "Leucocytes                   75.536583\n",
      "CRP                          77.028644\n",
      "LacticAcid                   89.794941\n",
      "remaining_time(days)          0.000000\n",
      "dtype: float64\n",
      "\n",
      "Column 'Age' has 93.81% missing values\n",
      "Dropping 'Age' column due to high percentage of missing values\n",
      "\n",
      "Column 'Leucocytes' has 75.54% missing values\n",
      "Dropping 'Leucocytes' column due to high percentage of missing values\n",
      "\n",
      "Column 'CRP' has 77.03% missing values\n",
      "Dropping 'CRP' column due to high percentage of missing values\n",
      "\n",
      "Column 'LacticAcid' has 89.79% missing values\n",
      "Dropping 'LacticAcid' column due to high percentage of missing values\n",
      "\n",
      "Column 'InfectionSuspected' has 93.81% missing values\n",
      "Dropping 'InfectionSuspected' column due to high percentage of missing values\n",
      "\n",
      "Column 'DiagnosticBlood' has 93.81% missing values\n",
      "Dropping 'DiagnosticBlood' column due to high percentage of missing values\n",
      "\n",
      "Column 'DisfuncOrg' has 93.81% missing values\n",
      "Dropping 'DisfuncOrg' column due to high percentage of missing values\n",
      "\n",
      "Column 'SIRSCritTachypnea' has 93.81% missing values\n",
      "Dropping 'SIRSCritTachypnea' column due to high percentage of missing values\n",
      "\n",
      "Column 'Hypotensie' has 93.81% missing values\n",
      "Dropping 'Hypotensie' column due to high percentage of missing values\n",
      "\n",
      "Column 'SIRSCritHeartRate' has 93.81% missing values\n",
      "Dropping 'SIRSCritHeartRate' column due to high percentage of missing values\n",
      "\n",
      "Column 'Infusion' has 93.81% missing values\n",
      "Dropping 'Infusion' column due to high percentage of missing values\n",
      "\n",
      "Column 'DiagnosticArtAstrup' has 93.81% missing values\n",
      "Dropping 'DiagnosticArtAstrup' column due to high percentage of missing values\n",
      "\n",
      "Column 'DiagnosticIC' has 93.81% missing values\n",
      "Dropping 'DiagnosticIC' column due to high percentage of missing values\n",
      "\n",
      "Column 'DiagnosticSputum' has 93.81% missing values\n",
      "Dropping 'DiagnosticSputum' column due to high percentage of missing values\n",
      "\n",
      "Column 'DiagnosticLiquor' has 93.81% missing values\n",
      "Dropping 'DiagnosticLiquor' column due to high percentage of missing values\n",
      "\n",
      "Column 'DiagnosticOther' has 93.81% missing values\n",
      "Dropping 'DiagnosticOther' column due to high percentage of missing values\n",
      "\n",
      "Column 'SIRSCriteria2OrMore' has 93.81% missing values\n",
      "Dropping 'SIRSCriteria2OrMore' column due to high percentage of missing values\n",
      "\n",
      "Column 'DiagnosticXthorax' has 93.81% missing values\n",
      "Dropping 'DiagnosticXthorax' column due to high percentage of missing values\n",
      "\n",
      "Column 'SIRSCritTemperature' has 93.81% missing values\n",
      "Dropping 'SIRSCritTemperature' column due to high percentage of missing values\n",
      "\n",
      "Column 'DiagnosticUrinaryCulture' has 93.81% missing values\n",
      "Dropping 'DiagnosticUrinaryCulture' column due to high percentage of missing values\n",
      "\n",
      "Column 'SIRSCritLeucos' has 93.81% missing values\n",
      "Dropping 'SIRSCritLeucos' column due to high percentage of missing values\n",
      "\n",
      "Column 'Oligurie' has 93.81% missing values\n",
      "Dropping 'Oligurie' column due to high percentage of missing values\n",
      "\n",
      "Column 'DiagnosticLacticAcid' has 93.81% missing values\n",
      "Dropping 'DiagnosticLacticAcid' column due to high percentage of missing values\n",
      "\n",
      "Column 'Diagnose' has 93.85% missing values\n",
      "Dropping 'Diagnose' column due to high percentage of missing values\n",
      "\n",
      "Column 'Hypoxie' has 93.81% missing values\n",
      "Dropping 'Hypoxie' column due to high percentage of missing values\n",
      "\n",
      "Column 'DiagnosticUrinarySediment' has 93.81% missing values\n",
      "Dropping 'DiagnosticUrinarySediment' column due to high percentage of missing values\n",
      "\n",
      "Column 'DiagnosticECG' has 93.81% missing values\n",
      "Dropping 'DiagnosticECG' column due to high percentage of missing values\n",
      "\n",
      "Column 'Variant' has 0.00% missing values\n",
      "Filled missing values in 'Variant' with mode value 'Variant 608'\n",
      "\n",
      "Column 'Variant index' has 0.00% missing values\n",
      "Filled missing values in 'Variant index' with mode value '608'\n",
      "\n",
      "Column 'lifecycle:transition' has 0.00% missing values\n",
      "Filled missing values in 'lifecycle:transition' with mode value 'complete'\n",
      "\n",
      "Column 'org:group' has 0.00% missing values\n",
      "Filled missing values in 'org:group' with mode value 'B'\n",
      "\n",
      "One-hot encoding 'org:group' column\n",
      "\n",
      "One-hot encoding 'Variant' column\n",
      "\n",
      "Processing 'Variant index' column\n",
      "Filled missing values in 'Variant index' with median value 362.0\n",
      "\n",
      "Data after preprocessing:\n",
      "  Case ID         Activity  Complete Timestamp  Variant index  \\\n",
      "0       A  ER Registration 2014-10-22 11:15:41             35   \n",
      "1       A       Leucocytes 2014-10-22 11:27:00             35   \n",
      "2       A              CRP 2014-10-22 11:27:00             35   \n",
      "3       A       LacticAcid 2014-10-22 11:27:00             35   \n",
      "4       A        ER Triage 2014-10-22 11:33:37             35   \n",
      "\n",
      "  lifecycle:transition  remaining_time(days)  org_group_A  org_group_B  \\\n",
      "0             complete             11.166192         True        False   \n",
      "1             complete             11.158333        False         True   \n",
      "2             complete             11.158333        False         True   \n",
      "3             complete             11.158333        False         True   \n",
      "4             complete             11.153738        False        False   \n",
      "\n",
      "   org_group_C  org_group_D  ...  Variant_Variant 90  Variant_Variant 91  \\\n",
      "0        False        False  ...               False               False   \n",
      "1        False        False  ...               False               False   \n",
      "2        False        False  ...               False               False   \n",
      "3        False        False  ...               False               False   \n",
      "4         True        False  ...               False               False   \n",
      "\n",
      "   Variant_Variant 92  Variant_Variant 93  Variant_Variant 94  \\\n",
      "0               False               False               False   \n",
      "1               False               False               False   \n",
      "2               False               False               False   \n",
      "3               False               False               False   \n",
      "4               False               False               False   \n",
      "\n",
      "   Variant_Variant 95  Variant_Variant 96  Variant_Variant 97  \\\n",
      "0               False               False               False   \n",
      "1               False               False               False   \n",
      "2               False               False               False   \n",
      "3               False               False               False   \n",
      "4               False               False               False   \n",
      "\n",
      "   Variant_Variant 98  Variant_Variant 99  \n",
      "0               False               False  \n",
      "1               False               False  \n",
      "2               False               False  \n",
      "3               False               False  \n",
      "4               False               False  \n",
      "\n",
      "[5 rows x 758 columns]\n"
     ]
    }
   ],
   "source": [
    "### 2.1 Data preprocessing\n",
    "\n",
    "# TODO: decide on how you handle missing values (ranging from dropping columns and dropping rows to filling in the empty cells)\n",
    "\n",
    "# Check for missing values per column\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Calculate percentage of missing values per column\n",
    "missing_percentage = df.isnull().sum() * 100 / len(df)\n",
    "print(\"\\nPercentage of missing values per column:\")\n",
    "print(missing_percentage)\n",
    "\n",
    "# Essential columns: 'Case ID', 'Activity', 'Complete Timestamp', 'remaining_time(days)'\n",
    "# Drop rows where these columns have missing values\n",
    "df = df.dropna(subset=['Case ID', 'Activity', 'Complete Timestamp', 'remaining_time(days)'])\n",
    "\n",
    "# Ensure 'Complete Timestamp' is in datetime format\n",
    "df['Complete Timestamp'] = pd.to_datetime(df['Complete Timestamp'])\n",
    "\n",
    "# Now, handle missing values column by column, with detailed explanations\n",
    "\n",
    "# List of numerical columns\n",
    "numerical_cols = ['Age', 'Leucocytes', 'CRP', 'LacticAcid']\n",
    "\n",
    "# List of binary columns (should be True/False or 1/0)\n",
    "binary_cols = ['InfectionSuspected', 'DiagnosticBlood', 'DisfuncOrg', 'SIRSCritTachypnea', 'Hypotensie',\n",
    "               'SIRSCritHeartRate', 'Infusion', 'DiagnosticArtAstrup', 'DiagnosticIC', 'DiagnosticSputum',\n",
    "               'DiagnosticLiquor', 'DiagnosticOther', 'SIRSCriteria2OrMore', 'DiagnosticXthorax',\n",
    "               'SIRSCritTemperature', 'DiagnosticUrinaryCulture', 'SIRSCritLeucos', 'Oligurie',\n",
    "               'DiagnosticLacticAcid', 'Diagnose', 'Hypoxie', 'DiagnosticUrinarySediment', 'DiagnosticECG']\n",
    "\n",
    "# List of categorical columns\n",
    "categorical_cols = ['Variant', 'Variant index', 'lifecycle:transition', 'org:group']\n",
    "\n",
    "# Handle numerical columns\n",
    "for col in numerical_cols:\n",
    "    if col in df.columns:\n",
    "        missing_pct = missing_percentage[col]\n",
    "        print(f\"\\nColumn '{col}' has {missing_pct:.2f}% missing values\")\n",
    "        if missing_pct > 50:\n",
    "            # Drop the column if more than 50% missing values\n",
    "            print(f\"Dropping '{col}' column due to high percentage of missing values\")\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "        else:\n",
    "            # Fill missing values with median\n",
    "            median_value = df[col].median()\n",
    "            df[col] = df[col].fillna(median_value)\n",
    "            print(f\"Filled missing values in '{col}' with median value {median_value}\")\n",
    "            # Convert to numeric type\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Handle binary columns\n",
    "for col in binary_cols:\n",
    "    if col in df.columns:\n",
    "        missing_pct = missing_percentage[col]\n",
    "        print(f\"\\nColumn '{col}' has {missing_pct:.2f}% missing values\")\n",
    "        if missing_pct > 50:\n",
    "            # Drop the column if more than 50% missing values\n",
    "            print(f\"Dropping '{col}' column due to high percentage of missing values\")\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "        else:\n",
    "            # Fill missing values with mode (most frequent value)\n",
    "            mode_value = df[col].mode()[0]\n",
    "            df[col] = df[col].fillna(mode_value)\n",
    "            print(f\"Filled missing values in '{col}' with mode value '{mode_value}'\")\n",
    "            # Convert to integer type\n",
    "            df[col] = df[col].astype(int)\n",
    "\n",
    "# Handle categorical columns\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        missing_pct = missing_percentage[col]\n",
    "        print(f\"\\nColumn '{col}' has {missing_pct:.2f}% missing values\")\n",
    "        if missing_pct > 50:\n",
    "            # Drop the column if more than 50% missing values\n",
    "            print(f\"Dropping '{col}' column due to high percentage of missing values\")\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "        else:\n",
    "            # Fill missing values with mode (most frequent value)\n",
    "            mode_value = df[col].mode()[0]\n",
    "            df[col] = df[col].fillna(mode_value)\n",
    "            print(f\"Filled missing values in '{col}' with mode value '{mode_value}'\")\n",
    "\n",
    "# Encode categorical variables\n",
    "# One-hot encode 'org:group' and 'Variant' if they exist\n",
    "if 'org:group' in df.columns:\n",
    "    print(\"\\nOne-hot encoding 'org:group' column\")\n",
    "    org_group_dummies = pd.get_dummies(df['org:group'], prefix='org_group')\n",
    "    df = pd.concat([df, org_group_dummies], axis=1)\n",
    "    df.drop(['org:group'], axis=1, inplace=True)\n",
    "\n",
    "if 'Variant' in df.columns:\n",
    "    print(\"\\nOne-hot encoding 'Variant' column\")\n",
    "    variant_dummies = pd.get_dummies(df['Variant'], prefix='Variant')\n",
    "    df = pd.concat([df, variant_dummies], axis=1)\n",
    "    df.drop(['Variant'], axis=1, inplace=True)\n",
    "\n",
    "# Process 'Variant index' column\n",
    "if 'Variant index' in df.columns:\n",
    "    print(\"\\nProcessing 'Variant index' column\")\n",
    "    df['Variant index'] = pd.to_numeric(df['Variant index'], errors='coerce')\n",
    "    missing_pct = df['Variant index'].isnull().sum() * 100 / len(df)\n",
    "    if missing_pct > 50:\n",
    "        print(f\"Dropping 'Variant index' column due to {missing_pct:.2f}% missing values\")\n",
    "        df.drop(columns=['Variant index'], inplace=True)\n",
    "    else:\n",
    "        median_value = df['Variant index'].median()\n",
    "        df['Variant index'] = df['Variant index'].fillna(median_value)\n",
    "        df['Variant index'] = df['Variant index'].astype(int)\n",
    "        print(f\"Filled missing values in 'Variant index' with median value {median_value}\")\n",
    "\n",
    "# Scale numerical features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Get the list of numerical columns that are still in df\n",
    "numerical_cols_in_df = [col for col in numerical_cols if col in df.columns]\n",
    "\n",
    "# Scale numerical columns\n",
    "if numerical_cols_in_df:\n",
    "    df[numerical_cols_in_df] = scaler.fit_transform(df[numerical_cols_in_df])\n",
    "    print(f\"\\nScaled numerical columns: {numerical_cols_in_df}\")\n",
    "\n",
    "# Save the resulting DataFrame to a CSV file\n",
    "output_filename = \"2.1.csv\"\n",
    "# df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(\"\\nData after preprocessing:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87db2911-a575-46a2-9210-e93bf988d9cd",
   "metadata": {},
   "source": [
    "### 2.2 Trace Encoding\n",
    "\n",
    "\n",
    "- Implement the aggregation encoding for the data set (for example, see [1], Table 6)\n",
    "\n",
    "<span style=\"color:gray\">[1] Ilya Verenich, Marlon Dumas, Marcello La Rosa, Fabrizio Maria Maggi, Irene Teinemaa:\n",
    "Survey and Cross-benchmark Comparison of Remaining Time Prediction Methods in Business Process Monitoring. ACM Trans. Intell. Syst. Technol. 10(4): 34:1-34:34 (2019) [Section 1, 2, 4.1, 4.3, 4.6, 5.2, 5.3, 5.4, and 6] </span>\n",
    "\n",
    "This encoding has been discussed during lecture 7.\n",
    "- for the aggregation encoding check the pandas groupby.DataFrameGroupBy and cumsum function and read the [examples and answers on the stake overflow](https://stackoverflow.com/a/49578219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "christian-street",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after trace encoding:\n",
      "  Case ID         Activity  Complete Timestamp  Variant index  \\\n",
      "0       A  ER Registration 2014-10-22 11:15:41             35   \n",
      "1       A       Leucocytes 2014-10-22 11:27:00             35   \n",
      "2       A              CRP 2014-10-22 11:27:00             35   \n",
      "3       A       LacticAcid 2014-10-22 11:27:00             35   \n",
      "4       A        ER Triage 2014-10-22 11:33:37             35   \n",
      "\n",
      "  lifecycle:transition  remaining_time(days)  org_group_A  org_group_B  \\\n",
      "0             complete             11.166192         True        False   \n",
      "1             complete             11.158333        False         True   \n",
      "2             complete             11.158333        False         True   \n",
      "3             complete             11.158333        False         True   \n",
      "4             complete             11.153738        False        False   \n",
      "\n",
      "   org_group_C  org_group_D  ...  act_ER Sepsis Triage  act_ER Triage  \\\n",
      "0        False        False  ...                     0              0   \n",
      "1        False        False  ...                     0              0   \n",
      "2        False        False  ...                     0              0   \n",
      "3        False        False  ...                     0              0   \n",
      "4         True        False  ...                     0              1   \n",
      "\n",
      "   act_IV Antibiotics  act_IV Liquid  act_LacticAcid  act_Leucocytes  \\\n",
      "0                   0              0               0               0   \n",
      "1                   0              0               0               1   \n",
      "2                   0              0               0               1   \n",
      "3                   0              0               1               1   \n",
      "4                   0              0               1               1   \n",
      "\n",
      "   act_Release A  act_Release C  act_Release D  act_Release E  \n",
      "0              0              0              0              0  \n",
      "1              0              0              0              0  \n",
      "2              0              0              0              0  \n",
      "3              0              0              0              0  \n",
      "4              0              0              0              0  \n",
      "\n",
      "[5 rows x 773 columns]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement the function that returns the aggregation state encoding of a log\n",
    "def agg_per_event_encoding(dataFrame, columnCase, columnActivity, numerical_cols):\n",
    "    _new_data = dataFrame.copy()\n",
    "    \n",
    "    # TODO: Apply one-hot encoding to the 'Activity' column\n",
    "    # One-hot encode the 'Activity' column to create activity features\n",
    "    onehot = pd.get_dummies(_new_data[columnActivity], prefix='act')\n",
    "    \n",
    "    # TODO: Group by 'case id' and compute the cumulative sum for each activity\n",
    "    # Compute cumulative counts of activities per case\n",
    "    cumulative_freq = onehot.groupby(_new_data[columnCase]).cumsum()\n",
    "    \n",
    "    # Compute time since case start for each event\n",
    "    _new_data['time_since_case_start'] = _new_data.groupby(columnCase)['Complete Timestamp'].transform(lambda x: x - x.min())\n",
    "    _new_data['time_since_case_start'] = _new_data['time_since_case_start'].dt.total_seconds() / 3600  # in hours\n",
    "    \n",
    "    # Compute cumulative averages of numerical variables per case\n",
    "    cumulative_numerical = _new_data.groupby(columnCase)[numerical_cols].expanding().mean().reset_index(level=0, drop=True)\n",
    "    cumulative_numerical.columns = [f'cumulative_mean_{col}' for col in numerical_cols]\n",
    "    \n",
    "    # TODO: Concatenate the original DataFrame with the cumulative frequencies and cumulative numerical features\n",
    "    _data_with_features = pd.concat([_new_data.reset_index(drop=True), cumulative_freq.reset_index(drop=True), cumulative_numerical.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    return _data_with_features\n",
    "\n",
    "# For the data set, apply the aggregated state encoding\n",
    "numerical_cols_in_df = [col for col in numerical_cols if col in df.columns]\n",
    "data_Sepsis_ag = agg_per_event_encoding(df, column_Sepsis_CaseID, column_Sepsis_Activity, numerical_cols_in_df)\n",
    "print(\"\\nData after trace encoding:\")\n",
    "print(data_Sepsis_ag.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-complaint",
   "metadata": {},
   "source": [
    "### 2.3 Create training and test data sets using temporal split\n",
    "\n",
    "\n",
    "Choose the size of your test data and use that to find the appropiate date (time threshold) to split the dataset into training  and test set.\n",
    "\n",
    "This approach is commonly used for time-series or event log data to ensure that training data comes from earlier time periods and test data from later periods. This avoids data leakage, where future data might influence the training process.\n",
    "\n",
    "When writing your report, explain how you split the data and provide a justification for your choice as part of the experiment setup discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f0f4b5b0-9997-4c41-9567-46829680f040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training features:\n",
      "    Variant index lifecycle:transition  org_group_A  org_group_B  org_group_C  \\\n",
      "32             37             complete         True        False        False   \n",
      "33             37             complete        False        False         True   \n",
      "34             37             complete         True        False        False   \n",
      "35             37             complete        False         True        False   \n",
      "36             37             complete        False         True        False   \n",
      "\n",
      "    org_group_D  org_group_E  org_group_F  org_group_G  org_group_H  ...  \\\n",
      "32        False        False        False        False        False  ...   \n",
      "33        False        False        False        False        False  ...   \n",
      "34        False        False        False        False        False  ...   \n",
      "35        False        False        False        False        False  ...   \n",
      "36        False        False        False        False        False  ...   \n",
      "\n",
      "    act_ER Sepsis Triage  act_ER Triage  act_IV Antibiotics  act_IV Liquid  \\\n",
      "32                     0              0                   0              0   \n",
      "33                     0              1                   0              0   \n",
      "34                     1              1                   0              0   \n",
      "35                     1              1                   0              0   \n",
      "36                     1              1                   0              0   \n",
      "\n",
      "    act_LacticAcid  act_Leucocytes  act_Release A  act_Release C  \\\n",
      "32               0               0              0              0   \n",
      "33               0               0              0              0   \n",
      "34               0               0              0              0   \n",
      "35               0               1              0              0   \n",
      "36               0               1              0              0   \n",
      "\n",
      "    act_Release D  act_Release E  \n",
      "32              0              0  \n",
      "33              0              0  \n",
      "34              0              0  \n",
      "35              0              0  \n",
      "36              0              0  \n",
      "\n",
      "[5 rows x 769 columns]\n",
      "\n",
      "Training labels:\n",
      "32    5.604525\n",
      "33    5.579595\n",
      "34    5.579479\n",
      "35    5.579167\n",
      "36    5.579167\n",
      "Name: remaining_time(days), dtype: float64\n",
      "\n",
      "Test features:\n",
      "   Variant index lifecycle:transition  org_group_A  org_group_B  org_group_C  \\\n",
      "0             35             complete         True        False        False   \n",
      "1             35             complete        False         True        False   \n",
      "2             35             complete        False         True        False   \n",
      "3             35             complete        False         True        False   \n",
      "4             35             complete        False        False         True   \n",
      "\n",
      "   org_group_D  org_group_E  org_group_F  org_group_G  org_group_H  ...  \\\n",
      "0        False        False        False        False        False  ...   \n",
      "1        False        False        False        False        False  ...   \n",
      "2        False        False        False        False        False  ...   \n",
      "3        False        False        False        False        False  ...   \n",
      "4        False        False        False        False        False  ...   \n",
      "\n",
      "   act_ER Sepsis Triage  act_ER Triage  act_IV Antibiotics  act_IV Liquid  \\\n",
      "0                     0              0                   0              0   \n",
      "1                     0              0                   0              0   \n",
      "2                     0              0                   0              0   \n",
      "3                     0              0                   0              0   \n",
      "4                     0              1                   0              0   \n",
      "\n",
      "   act_LacticAcid  act_Leucocytes  act_Release A  act_Release C  \\\n",
      "0               0               0              0              0   \n",
      "1               0               1              0              0   \n",
      "2               0               1              0              0   \n",
      "3               1               1              0              0   \n",
      "4               1               1              0              0   \n",
      "\n",
      "   act_Release D  act_Release E  \n",
      "0              0              0  \n",
      "1              0              0  \n",
      "2              0              0  \n",
      "3              0              0  \n",
      "4              0              0  \n",
      "\n",
      "[5 rows x 769 columns]\n",
      "\n",
      "Test labels:\n",
      "0    11.166192\n",
      "1    11.158333\n",
      "2    11.158333\n",
      "3    11.158333\n",
      "4    11.153738\n",
      "Name: remaining_time(days), dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define time threshold\n",
    "time_threshold = pd.Timestamp('2014-09-30 00:00:00')\n",
    "\n",
    "# Split the case \n",
    "def train_test_time_based_split(data_frame, time_threshold, column_case_id, column_activity, column_time_stamp, column_label):\n",
    "    # TODO: Identify the start time of each case\n",
    "    case_start_times = data_frame.groupby(column_case_id)[column_time_stamp].min().reset_index()\n",
    "    case_start_times.columns = [column_case_id, 'case_start_time']\n",
    "    \n",
    "    # Merge start times back into the data_frame\n",
    "    data_frame = data_frame.merge(case_start_times, on=column_case_id, how='left')\n",
    "    \n",
    "    # TODO: Separate case IDs into training and test sets\n",
    "    train_case_ids = case_start_times[case_start_times['case_start_time'] < time_threshold][column_case_id]\n",
    "    test_case_ids = case_start_times[case_start_times['case_start_time'] >= time_threshold][column_case_id]\n",
    "    \n",
    "    # TODO: Assign rows to training and test sets based on case IDs\n",
    "    train_data = data_frame[data_frame[column_case_id].isin(train_case_ids)]\n",
    "    test_data = data_frame[data_frame[column_case_id].isin(test_case_ids)]\n",
    "    \n",
    "    # TODO : Create the training and test sets, while dropping the irrelevant columns \n",
    "    irrelevant_columns = [column_case_id, column_activity, column_time_stamp, column_label, 'case_start_time']\n",
    "    feature_columns = [col for col in data_frame.columns if col not in irrelevant_columns]\n",
    "    \n",
    "    X_train = train_data[feature_columns]\n",
    "    y_train = train_data[column_label]\n",
    "    \n",
    "    X_test = test_data[feature_columns]\n",
    "    y_test = test_data[column_label]\n",
    "    \n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Apply the time-based split to the encoded dataset\n",
    "X_train, X_test, y_train, y_test = train_test_time_based_split(\n",
    "    data_Sepsis_ag, time_threshold, column_Sepsis_CaseID, \n",
    "    column_Sepsis_Activity, column_Sepsis_Timestamps, label_column)\n",
    "\n",
    "print(\"\\nTraining features:\")\n",
    "print(X_train.head())\n",
    "print(\"\\nTraining labels:\")\n",
    "print(y_train.head())\n",
    "\n",
    "print(\"\\nTest features:\")\n",
    "print(X_test.head())\n",
    "print(\"\\nTest labels:\")\n",
    "print(y_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-quantum",
   "metadata": {},
   "source": [
    "## Task 3: Predicting Case Remaining Time \n",
    "\n",
    "\n",
    "In this task, you will train a regression model (aka regressor) to predict case remaining time. \n",
    "You may choose the regression tree, the random forest regression, the kNN-regressor, or the MLP for regression. Very similar to how you have trained a classification model in Assignment 1, now perform the following steps to train a regression model. \n",
    "\n",
    "A) use the default values for the parameters to get a regressor on the training data. \n",
    "- [Regression Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)\n",
    "- [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "\n",
    "(OPTIONAL) use 5-fold cross-validation to tune the parameters. And create plots to show how the selected parameters affect the performance. \n",
    "\n",
    "B) select the best-performing regressor (e.g., the default one or the one that achieved the lowest error) and report the error measures (MAE, MSE, RMSE, R^2) of the fitted model on the test data. \n",
    "\n",
    "    \n",
    "#### TIPS:\n",
    "In case you decide to perform cross-validation, you are allowed to reuse some of your code from Assignment 1 or use the [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) class (see an [example](https://www.dezyre.com/recipes/find-optimal-parameters-using-gridsearchcv-for-regression), but be aware that GridSearchSV does not return MAE or the other error measures (e.g., MSE, RMSE, R^2), you will need to update the scoring function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "raising-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import packages\n",
    "\n",
    "\n",
    "# TODO: Train a regression model (e.g., Random Forest Regressor)\n",
    "\n",
    "\n",
    "# TODO: Evaluate the regressor on the test data and print the errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef067c7f-fc9a-4852-a13a-409032ca3d38",
   "metadata": {},
   "source": [
    "## Task 4. Create three buckets and repeat Tasks 2 and 3 for each bucket. \n",
    "\n",
    "In this task, you will create three buckets, for prefix length 5, 10 and 15. For each of the bucket, repeat Task 2 and 3.  \n",
    "\n",
    "You may use the functions you built for Tasks 2 and 3 or reuse code. \n",
    "\n",
    "Calculate the error measures (MAE, MSE, RMSE, R^2) and discuss the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4749491b-1ac3-4800-a2fd-5e012b52274f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'empty'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m regressors \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prefix_length, bucket_df \u001b[38;5;129;01min\u001b[39;00m buckets\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mbucket_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m:\n\u001b[0;32m     21\u001b[0m         \n\u001b[0;32m     22\u001b[0m         \u001b[38;5;66;03m# TODO: Apply the encoding\u001b[39;00m\n\u001b[0;32m     23\u001b[0m         bucket_df_Sepsis_ag \u001b[38;5;241m=\u001b[39m agg_per_event_encoding(bucket_df, column_Sepsis_CaseID, column_Sepsis_Activity)\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;66;03m# TODO: Retain the event of prefix_length\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'empty'"
     ]
    }
   ],
   "source": [
    "# TODO: import packages\n",
    "\n",
    "\n",
    "# Group by case_id and create a prefix column\n",
    "df['prefix'] = df.groupby(column_Sepsis_CaseID).cumcount() + 1\n",
    "\n",
    "# Buckets for prefix lengths 5, 10, and 15\n",
    "buckets = {5: [], 10: [], 15:[]}\n",
    "\n",
    "# TODO: Populate the buckets\n",
    "for prefix_length in buckets.keys():\n",
    "    # TODO: Populate the bucket with the events that have prefix <= prefix_length\n",
    "    ...\n",
    "\n",
    "# Create a regressor for each bucket    \n",
    "regressors = {}\n",
    "\n",
    "\n",
    "for prefix_length, bucket_df in buckets.items():\n",
    "    if not bucket_df.empty:\n",
    "        \n",
    "        # TODO: Apply the encoding\n",
    "        bucket_df_Sepsis_ag = agg_per_event_encoding(bucket_df, column_Sepsis_CaseID, column_Sepsis_Activity)\n",
    "        \n",
    "        # TODO: Retain the event of prefix_length\n",
    "        bucket_df_Sepsis_ag = bucket_df_Sepsis_ag[bucket_df_Sepsis_ag['prefix'] == prefix_length]\n",
    "       \n",
    "        # TODO: Create train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_time_based_split(\n",
    "            bucket_df_Sepsis_ag, time_threshold, column_Sepsis_CaseID, \n",
    "            column_Sepsis_Activity, column_Sepsis_Timestamps, label_column)\n",
    "\n",
    "        # TODO: Train a regression model (e.g., Random Forest Regressor)\n",
    "\n",
    "\n",
    "        # TODO: Evaluate the regressor by calculating the MAE, etc...\n",
    "\n",
    "        # TODO: Store/print your results\n",
    "\n",
    "        # Store the trained regressor\n",
    "        regressors[prefix_length] = reg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-apache",
   "metadata": {},
   "source": [
    "## Task 5. Two Alternative Preprocessing and Encoding Methods\n",
    "\n",
    "In this task, you will refine the design of your method so far and compare the performance of different preprocessing and encoding methods. **Choose two of the following options**:\n",
    "\n",
    "A) If you have dropped all features except the *activities* in Task 2, select a few features (e.g., Age, Leukocytes, CRP, Lactic Acid), encode them, and repeat Tasks 2 and 3. Motivate your selection in your report. \n",
    "\n",
    "B) If you already included some features in Task 2, drop all features except the encoded *activities*, and repeat Task 3.\n",
    "\n",
    "C) Engineer a feature called *elapsed time* by computing the time elapsed since the case started until the current event, and repeat Tasks 2 and 3. Evaluate if adding this feature (*elapsed time*) help improve the model performance. \n",
    "\n",
    "D) Use *last-state encoding* instead of aggregation encoding, and repeat Tasks 2 and 3. Evaluate if the *last-state encoding* help improve the model performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-workshop",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adult-equilibrium",
   "metadata": {},
   "source": [
    "## Task 6.  Report your results and discuss your findings\n",
    "\n",
    "By now, you have trained and applied a regressor and evaluated its performance on \n",
    "(1) the non-bucketed training and test set with all possible prefix length. \n",
    "(2) three buckets of different prefix length. \n",
    "(3 & 4) and two other methods you tried (e.g., a different encoding or with additional features). \n",
    "\n",
    "You have created tables or figures which you can add to your report. \n",
    "\n",
    "Create an overview table or figure that compares the performance of each method on the data set, for example, see the table here below. \n",
    "\n",
    "\n",
    "Discuss your findings and reflect on the following questions in your report:\n",
    "- According to the error measures, which one would you suggest as the optimal method (preprocessing + encoding + algorithm)? \n",
    "- Are there any discrepancies between the MAE, MSE, RMSE, and R^2 measures in terms of which model/method performs the best? If yes, how would you explain these discrepancies. \n",
    "- Which one of the MAE, MSE, RMSE, and R^2 would you use for selecting the model? Why?\n",
    "- Which one of the encoding would you suggest for this data set? Why?\n",
    "- Which features have a big influence on predicting the remaining time?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Encoding | Model | Training MAE  | Test MAE |  Training MSE  |  Test MSE  | Training R^2 | Test R^2 |... |\n",
    "|------|------|------|------|------|------|------|------|-----|\n",
    "|  Agg-state and no bucketing |  RF regressor |  |  | || | |\n",
    "|  Agg-state and prefix length 5 |RF regressor       |  |  | || | |\n",
    "|   Agg-state and prefix length 10 |RF regressor     |  |  | || | |\n",
    "|   Agg-state and prefix length 15 |RF regressor     |  |  | || | |\n",
    "|   last-state |RF regressor    |  |  | || | |\n",
    "|   last-state + additional features |RF regressor    |  |  | || | |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba6a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82de0533",
   "metadata": {},
   "source": [
    "## Bonus Tasks \n",
    "\n",
    "We would like to challenge you with the following bonus tasks. For each task that is successfully completed, you may obtain max. 0.5 extra point added to the total 14 points. \n",
    "\n",
    "1. Train a MLP for regression. (If you have used MLP for Task 3, then train another regressor of interest. Evaluate the performance. Explain this in your report.)\n",
    "2. Train an Autoencoder for feature reduction/learning and evaluate whether it helps improve the performance. Explain this in your report.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02745358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
